{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import torchvision.transforms as T\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, ViTForImageClassification, ViTFeatureExtractor, CLIPProcessor, CLIPModel, AutoFeatureExtractor, get_cosine_schedule_with_warmup, ViTModel\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_dir = 'train'\n",
    "test_dir = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIPモデルとプロセッサのロード\n",
    "clip_model_name = \"openai/clip-vit-large-patch14\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'hold'フォルダの擬似ラベル用テキストプロンプト\n",
    "hold_text_prompts = [\n",
    "    \"a person holding an open fan\",               # クラス 0\n",
    "    \"a person holding a closed fan\"               # クラス 1\n",
    "]\n",
    "\n",
    "# 'not-hold'フォルダの擬似ラベル用テキストプロンプト\n",
    "not_hold_text_prompts = [\n",
    "    \"a fan is present but not held by the person\",  # クラス 2\n",
    "    \"no fan is present in the image\"                # クラス 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pseudo_labels(image_dir, image_files, text_prompts, class_offset=0):\n",
    "    \"\"\"\n",
    "    CLIPを用いて擬似ラベルを生成する関数\n",
    "    \n",
    "    Args:\n",
    "        image_dir (str): 画像ディレクトリのパス\n",
    "        image_files (list): 画像ファイル名のリスト\n",
    "        text_prompts (list): クラスごとのテキストプロンプトのリスト\n",
    "        class_offset (int): クラス番号のオフセット（'not-hold'フォルダ用に2を設定）\n",
    "    \n",
    "    Returns:\n",
    "        list: 擬似ラベルのリスト\n",
    "    \"\"\"\n",
    "    pseudo_labels = []\n",
    "    for img_file in tqdm(image_files, desc=f\"Generating pseudo labels for {os.path.basename(image_dir)}\"):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        inputs = clip_processor(text=text_prompts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image  # [1, num_texts]\n",
    "            probs = logits_per_image.softmax(dim=1)      # [1, num_texts]\n",
    "            pred = torch.argmax(probs, dim=1).item()\n",
    "        pseudo_labels.append(pred + class_offset)\n",
    "    return pseudo_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'hold'フォルダ内の画像ファイルのリストを取得\n",
    "hold_dir = os.path.join(train_dir, 'hold')\n",
    "hold_files = [f for f in os.listdir(hold_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# 擬似ラベルの生成\n",
    "hold_pseudo_labels = generate_pseudo_labels(hold_dir, hold_files, hold_text_prompts, class_offset=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'not-hold'フォルダ内の画像ファイルのリストを取得\n",
    "not_hold_dir = os.path.join(train_dir, 'not-hold')\n",
    "not_hold_files = [f for f in os.listdir(not_hold_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# 擬似ラベルの生成（クラス番号を2,3に調整）\n",
    "not_hold_pseudo_labels = generate_pseudo_labels(not_hold_dir, not_hold_files, not_hold_text_prompts, class_offset=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'hold'のデータフレーム作成\n",
    "hold_df = pd.DataFrame({\n",
    "    'file': ['hold/' + f for f in hold_files],\n",
    "    'label': hold_pseudo_labels\n",
    "})\n",
    "\n",
    "# 'not-hold'のデータフレーム作成\n",
    "not_hold_df = pd.DataFrame({\n",
    "    'file': ['not-hold/' + f for f in not_hold_files],\n",
    "    'label': not_hold_pseudo_labels\n",
    "})\n",
    "\n",
    "# 訓練データ全体を結合\n",
    "train_df = pd.concat([hold_df, not_hold_df], ignore_index=True)\n",
    "\n",
    "# データをシャッフル\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 擬似ラベルをCSVに保存\n",
    "train_df.to_csv('pseudo_labels.csv', index=False)\n",
    "print(\"Pseudo labels saved to 'pseudo_labels.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlipDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor, transform=None, class_captions=None, max_length=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): ファイル名とラベルを含むデータフレーム\n",
    "            image_dir (str): 画像ディレクトリのパス\n",
    "            processor (BlipProcessor): BLIPのプロセッサ\n",
    "            transform (albumentations.Compose): 画像の変換パイプライン\n",
    "            class_captions (dict): クラスラベルとキャプションの対応辞書\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.processor = processor\n",
    "        self.class_captions = class_captions\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.loc[idx, 'file']\n",
    "        label = self.dataframe.loc[idx, 'label']\n",
    "        caption = self.class_captions[label]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "    \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "    \n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True\n",
    "            )\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIPモデルとプロセッサのロード\n",
    "blip_model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "blip_processor = BlipProcessor.from_pretrained(blip_model_name)\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(blip_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.RandomResizedCrop(224, 224, scale=(0.8, 1.0), p=1.0),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.3),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.3),\n",
    "])\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "])\n",
    "# クラスラベルと対応するキャプション\n",
    "class_captions = {\n",
    "    0: \"a person holding an open fan\",\n",
    "    1: \"a person holding a closed fan\",\n",
    "    2: \"a fan is present but not held by the person\",\n",
    "    3: \"no fan is present in the image\"\n",
    "}\n",
    "\n",
    "# 擬似ラベルの読み込み\n",
    "train_df = pd.read_csv('pseudo_labels.csv')\n",
    "\n",
    "# 訓練データと検証データに分割\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, stratify=train_df['label'], random_state=42)\n",
    "\n",
    "# データセットの作成\n",
    "train_dataset = BlipDataset(train_data, train_dir, blip_processor, transform=train_transforms, class_captions=class_captions)\n",
    "val_dataset = BlipDataset(val_data, train_dir, blip_processor, transform=val_transforms, class_captions=class_captions)\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d2a0710f68432aa697bf9ebccd341a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/3794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4054\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5f49bd18cb4fb5817ca130df5aab03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1:   0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1.3634\n",
      "Best model saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ccd023ed0e4c56a6d2a2618864bb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/3794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.3665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cda5e5a59c4f9fa3e0f878d9f57a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 2:   0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Validation Loss: 1.3623\n",
      "Best model saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d28d98b9d44b08a8613ca14eb964ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/3794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.3632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93aa6dea742d4cf6aee6ed37fe03779e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 3:   0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Validation Loss: 1.3630\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff08baafd334379a9e083d329b1a102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/3794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2.8642\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44dde4a82314501a8d3fa9450039d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 4:   0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Validation Loss: 1.4122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f6613ec7294092a6167679bfb0edb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/3794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.3884\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a67e3ab31d42cd9e7a29a40451e594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 5:   0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Validation Loss: 1.3623\n"
     ]
    }
   ],
   "source": [
    "# オプティマイザの設定\n",
    "optimizer = AdamW(blip_model.parameters(), lr=5e-5)\n",
    "\n",
    "# ファインチューニングの実施\n",
    "best_val_loss = float('inf')\n",
    "epochs = 5\n",
    "blip_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    blip_model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "        # 生成キャプションをターゲットとして損失を計算\n",
    "        outputs = blip_model(input_ids=input_ids,\n",
    "                            pixel_values=pixel_values,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=input_ids)\n",
    "    \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # 検証ステップ（オプション）\n",
    "    blip_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "            outputs = blip_model(input_ids=input_ids,\n",
    "                                pixel_values=pixel_values,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=input_ids)\n",
    "    \n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # ベストモデルの保存（検証損失が低い場合）\n",
    "    if epoch == 0 or avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        blip_model.save_pretrained(\"finetuned_blip\")\n",
    "        blip_processor.save_pretrained(\"finetuned_blip_processor\")\n",
    "        print(\"Best model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ファインチューニング済みモデルのロード\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"finetuned_blip\").to(device)\n",
    "blip_processor = BlipProcessor.from_pretrained(\"finetuned_blip_processor\")\n",
    "blip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBlipDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): ファイル名を含むデータフレーム\n",
    "            image_dir (str): 画像ディレクトリのパス\n",
    "            processor (BlipProcessor): BLIPのプロセッサ\n",
    "            transform (albumentations.Compose): 画像の変換パイプライン\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.loc[idx, 'file']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "    \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "    \n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs, img_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b7552c8e384f0f826378e440c8f209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating captions for test data:   0%|          | 0/2144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# テストデータのファイルリストを取得\n",
    "test_files = [f for f in os.listdir(test_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "test_df = pd.DataFrame({'file': test_files})\n",
    "\n",
    "# テストデータセットとデータローダーの作成\n",
    "test_dataset = TestBlipDataset(test_df, test_dir, blip_processor, transform=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# ファインチューニング済みのBLIPモデルとプロセッサのロード\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"finetuned_blip\").to(device)\n",
    "blip_processor = BlipProcessor.from_pretrained(\"finetuned_blip_processor\")\n",
    "blip_model.eval()\n",
    "\n",
    "# キャプション生成\n",
    "captions = []\n",
    "image_names = []\n",
    "\n",
    "for batch in tqdm(test_loader, desc=\"Generating captions for test data\"):\n",
    "    inputs, img_names = batch\n",
    "    # input_ids = inputs['input_ids'].to(device)\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    attention_mask = inputs.get('attention_mask', None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = blip_model.generate(pixel_values=pixel_values, attention_mask=attention_mask, max_length=20)\n",
    "        generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    captions.extend(generated_caption)\n",
    "    image_names.extend(img_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a person holding an open fan                   18324\n",
       "a fan is present but not held by the person    15846\n",
       "no fan is present in the image                   129\n",
       "Name: caption, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_df = pd.DataFrame({\n",
    "    'file': image_names,\n",
    "    'caption': captions\n",
    "})\n",
    "cap_df['caption'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_caption_to_label(caption):\n",
    "    caption = caption.lower()\n",
    "    # 各クラスのキーワードを定義\n",
    "    if \"open fan\" in caption and \"holding\" in caption and \"not\" not in caption:\n",
    "        return 1  # \"a person holding an open fan\"\n",
    "    elif \"closed fan\" in caption and \"holding\" in caption and \"not\" not in caption:\n",
    "        return 1  # \"a person holding a closed fan\"\n",
    "    elif \"fan\" in caption and \"not held\" in caption:\n",
    "        return 0  # \"a fan is present but not held by the person\"\n",
    "    elif \"no fan\" in caption:\n",
    "        return 0  # \"no fan is present in the image\"\n",
    "    else:\n",
    "        # デフォルトで最も近いクラスを推定\n",
    "        print(f\"Unknown caption: {caption}\")\n",
    "        return 3\n",
    "\n",
    "# キャプションからラベルへのマッピング\n",
    "test_labels = [map_caption_to_label(c) for c in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' has been saved.\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'file': image_names,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False, header=False)\n",
    "print(\"Submission file 'submission.csv' has been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mYou have successfully submitted your predictions.We will send you the submission result to your email address.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!signate submit --competition-id=1506 submission.csv --note \"BLIP Train pseudo labels\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
